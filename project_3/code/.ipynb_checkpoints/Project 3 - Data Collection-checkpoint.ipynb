{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project seeks to identify the best classification model that can distinguish which of two subreddits a post belongs to. The  performance of 2 parametric models will be evaluated. In the instance of Logistic Regression, the coefficients can be interpreted to identify features that increases the log-odds of a post to be from the positive class, or else being equal.\n",
    "\n",
    "Advertisors seeking to use reddit as a marketing platform will be able to develop marketing strategies specifically targeted  at each subreddit based on what each community is most concerned about.\n",
    "\n",
    "Two closely-related subreddits, namely /gainit and /keto are selected to identify the classification model that can more accurately classify the posts to their respective subreddit. \n",
    "\n",
    "The /gainit subreddit is a fitness subreddit for information and discussion for people looking to put on weight, muscle, and strength.\n",
    "\n",
    "The /keto subreddit is a place to share thoughts, ideas, benefits, and experiences around eating within a Ketogenic lifestyle.\n",
    "\n",
    "While seemingly dissimilar, the 2 threads share commonalities. \n",
    "1. Focus on caloric intake and fitness\n",
    "2. Macronutrients: The basis of a ketogenic diet is one that focuses on a relatively high-protein, high-fat diet. The increased intake amount of these nutrients are common to what a person seeking to gain muscle/weight would incorporate to attain the one's goal\n",
    "\n",
    "Moderators of the post can use the model to potentially flag out misclassified posts to maintain the integrity of the posts in the subreddit thread.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the requests library, data is pulled via the Reddit's API. Reddit only provides 25 posts per request. In order to get 500 posts per subreddit, the process was iterated through 20 times- a buffer to cater for duplicate posts. A list of nested json dictionaries is obtained of which are saved in 2 separate csv files for EDA and modelling in a separate notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.reddit.com/r/keto.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from tqdm import tnrange,tqdm_notebook as tqdm\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jocelynpok/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb04993047ef4700a01c09536af3f3de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='loop', max=20.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.reddit.com/r/keto.json\n",
      "4\n",
      "https://www.reddit.com/r/keto.json?after=t3_gj1ps1\n",
      "2\n",
      "https://www.reddit.com/r/keto.json?after=t3_gic4s0\n",
      "2\n",
      "https://www.reddit.com/r/keto.json?after=t3_gimmqs\n",
      "2\n",
      "https://www.reddit.com/r/keto.json?after=t3_gicedv\n",
      "3\n",
      "https://www.reddit.com/r/keto.json?after=t3_gi0r8p\n",
      "6\n",
      "https://www.reddit.com/r/keto.json?after=t3_ghi7ou\n",
      "6\n",
      "https://www.reddit.com/r/keto.json?after=t3_ghcw3k\n",
      "4\n",
      "https://www.reddit.com/r/keto.json?after=t3_ggxv49\n",
      "3\n",
      "https://www.reddit.com/r/keto.json?after=t3_ggs7zu\n",
      "2\n",
      "https://www.reddit.com/r/keto.json?after=t3_gge4la\n",
      "2\n",
      "https://www.reddit.com/r/keto.json?after=t3_gg40to\n",
      "2\n",
      "https://www.reddit.com/r/keto.json?after=t3_gfrjnb\n",
      "3\n",
      "https://www.reddit.com/r/keto.json?after=t3_gfm23h\n",
      "6\n",
      "https://www.reddit.com/r/keto.json?after=t3_gfazst\n",
      "5\n",
      "https://www.reddit.com/r/keto.json?after=t3_gexvbe\n",
      "6\n",
      "https://www.reddit.com/r/keto.json?after=t3_gerzvo\n",
      "6\n",
      "https://www.reddit.com/r/keto.json?after=t3_gdeblc\n",
      "5\n",
      "https://www.reddit.com/r/keto.json?after=t3_gdvu3o\n",
      "5\n",
      "https://www.reddit.com/r/keto.json?after=t3_gdncr4\n",
      "5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "posts = []\n",
    "after = None\n",
    "#options = '&t=all&limit=100'\n",
    "my_list = list(range(20))\n",
    "#for a in tqdm(my_list):\n",
    "for a in tnrange(20,desc='loop'):\n",
    "    if after == None:\n",
    "        current_url = url\n",
    "    else:\n",
    "        current_url = url + '?after=' + after \n",
    "    print(current_url)\n",
    "    res = requests.get(current_url, headers={'User-agent': 'Pony Inc 1.0'})\n",
    "\n",
    "    if res.status_code != 200:\n",
    "        print('Status error', res.status_code)\n",
    "        break\n",
    "\n",
    "    current_dict = res.json()\n",
    "    current_posts = [p['data'] for p in current_dict['data']['children']]\n",
    "    posts.extend(current_posts)\n",
    "    after = current_dict['data']['after']\n",
    "\n",
    "    # COMPLETE THE CODE!\n",
    "    if a > 0:\n",
    "        prev_posts = pd.read_csv('./keto.csv')\n",
    "        current_df = pd.DataFrame(posts)\n",
    "        combined_posts = pd.concat([prev_posts,current_df], ignore_index=True)\n",
    "        combined_posts.to_csv('./keto.csv', index = False)\n",
    "\n",
    "    else:\n",
    "        pd.DataFrame(posts).to_csv('./keto.csv', index = False)\n",
    "\n",
    "    # generate a random sleep duration to look more 'natural'\n",
    "    sleep_duration = random.randint(2,6)\n",
    "    print(sleep_duration)\n",
    "    time.sleep(sleep_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5290"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keto_df = pd.read_csv('./keto.csv')\n",
    "keto_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.reddit.com/r/gainit.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jocelynpok/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64d1217b4fce4a1fbfd0c0a229073f5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='loop', max=20.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.reddit.com/r/gainit.json\n",
      "3\n",
      "https://www.reddit.com/r/gainit.json?after=t3_ghu04s\n",
      "2\n",
      "https://www.reddit.com/r/gainit.json?after=t3_gevnsq\n",
      "3\n",
      "https://www.reddit.com/r/gainit.json?after=t3_gdh8a1\n",
      "6\n",
      "https://www.reddit.com/r/gainit.json?after=t3_gca57e\n",
      "3\n",
      "https://www.reddit.com/r/gainit.json?after=t3_g7pbdn\n",
      "4\n",
      "https://www.reddit.com/r/gainit.json?after=t3_g63xtu\n",
      "2\n",
      "https://www.reddit.com/r/gainit.json?after=t3_g38a3c\n",
      "6\n",
      "https://www.reddit.com/r/gainit.json?after=t3_g1r03k\n",
      "2\n",
      "https://www.reddit.com/r/gainit.json?after=t3_fzp7c1\n",
      "5\n",
      "https://www.reddit.com/r/gainit.json?after=t3_fxel1s\n",
      "4\n",
      "https://www.reddit.com/r/gainit.json?after=t3_ful5x9\n",
      "4\n",
      "https://www.reddit.com/r/gainit.json?after=t3_frodeb\n",
      "5\n",
      "https://www.reddit.com/r/gainit.json?after=t3_fqo22i\n",
      "3\n",
      "https://www.reddit.com/r/gainit.json?after=t3_fozrp4\n",
      "3\n",
      "https://www.reddit.com/r/gainit.json?after=t3_fnx13j\n",
      "3\n",
      "https://www.reddit.com/r/gainit.json?after=t3_fmrsl5\n",
      "6\n",
      "https://www.reddit.com/r/gainit.json?after=t3_fjw9gn\n",
      "5\n",
      "https://www.reddit.com/r/gainit.json?after=t3_fikvgu\n",
      "5\n",
      "https://www.reddit.com/r/gainit.json?after=t3_fgzqt2\n",
      "6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "posts = []\n",
    "after = None\n",
    "#options = '&t=all&limit=100'\n",
    "my_list = list(range(20))\n",
    "for a in tnrange(20,desc='loop'):\n",
    "#for x in tqdm(my_list):\n",
    "#for a in range(20):\n",
    "    if after == None:\n",
    "        current_url = url\n",
    "    else:\n",
    "        current_url = url + '?after=' + after \n",
    "    print(current_url)\n",
    "    res = requests.get(current_url, headers={'User-agent': 'Pony Inc 1.0'})\n",
    "    \n",
    "    if res.status_code != 200:\n",
    "        print('Status error', res.status_code)\n",
    "        break\n",
    "    \n",
    "    current_dict = res.json()\n",
    "    current_posts = [p['data'] for p in current_dict['data']['children']]\n",
    "    posts.extend(current_posts)\n",
    "    after = current_dict['data']['after']\n",
    "    \n",
    "    # COMPLETE THE CODE!\n",
    "    if a > 0:\n",
    "        prev_posts = pd.read_csv('./gainit.csv')\n",
    "        current_df = pd.DataFrame(posts)\n",
    "        combined_posts = pd.concat([prev_posts,current_df], ignore_index=True)\n",
    "        combined_posts.to_csv('./gainit.csv', index = False)\n",
    "        \n",
    "    else:\n",
    "        pd.DataFrame(posts).to_csv('./gainit.csv', index = False)\n",
    "    \n",
    "    # generate a random sleep duration to look more 'natural'\n",
    "    sleep_duration = random.randint(2,6)\n",
    "    print(sleep_duration)\n",
    "    time.sleep(sleep_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5290"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gainIt_df = pd.read_csv('../datasets/gainit.csv')\n",
    "gainIt_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "keto_filtered_df = keto_df.loc[:,['selftext','title']]\n",
    "keto_filtered_df['target'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "gainIt_filtered_df = gainIt_df.loc[:,['selftext','title']]\n",
    "gainIt_filtered_df['target'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>New to gaining weight? Please read the FAQ bef...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>**Welcome to the weekly stupid questions threa...</td>\n",
       "      <td>[Mod] Simple Questions - the weekly stupid que...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yes, we understand not having everything be id...</td>\n",
       "      <td>Am I the only person who is tired of the const...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pics: https://m.imgur.com/a/itfwgds \\n\\nBeen t...</td>\n",
       "      <td>[PROGRESS] 71kg - 81 kg (157lbs-179lbs) 3 mont...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>That is all, just wanted to share. Feels good ...</td>\n",
       "      <td>Feel like I've finally gained enough weight to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            selftext  \\\n",
       "0                                                NaN   \n",
       "1  **Welcome to the weekly stupid questions threa...   \n",
       "2  Yes, we understand not having everything be id...   \n",
       "3  Pics: https://m.imgur.com/a/itfwgds \\n\\nBeen t...   \n",
       "4  That is all, just wanted to share. Feels good ...   \n",
       "\n",
       "                                               title  target  \n",
       "0  New to gaining weight? Please read the FAQ bef...       1  \n",
       "1  [Mod] Simple Questions - the weekly stupid que...       1  \n",
       "2  Am I the only person who is tired of the const...       1  \n",
       "3  [PROGRESS] 71kg - 81 kg (157lbs-179lbs) 3 mont...       1  \n",
       "4  Feel like I've finally gained enough weight to...       1  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df = pd.concat([keto_filtered_df,gainIt_filtered_df], ignore_index=True)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22120, 3)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1049, 3)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "selftext    45\n",
       "title        0\n",
       "target       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.fillna('missingtext', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.529075\n",
       "1    0.470925\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.target.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(combined_df[['selftext']],\n",
    "                                                    combined_df['target'],\n",
    "                                                    test_size = 0.25,\n",
    "                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selftext_to_words(raw):\n",
    "    # Function to convert a raw selftext to a string of words\n",
    "    # The input is a single string (a raw selftext), and \n",
    "    # the output is a single string (a preprocessed selftext)\n",
    "    \n",
    "    # 1. Remove HTML.\n",
    "    #review_text = BeautifulSoup(raw_review).get_text()\n",
    "    \n",
    "    # 2. Remove non-letters and leakage terms\n",
    "    pattern_1 = '\\[(.*?)\\]' #square brackets\n",
    "    pattern_2 = '(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})' #urls\n",
    "    pattern_3 = 'keto' #subreddit(prevent data leakage)\n",
    "    pattern_4 = '\\/r\\/' #indicative of subreddit\n",
    "    pattern_5 = 'gainit'\n",
    "    generic_re = re.compile(\"(%s|%s|%s|%s|%s)\" % (pattern_1, pattern_2, pattern_3,pattern_4,pattern_5))\n",
    "    raw_text = re.sub(generic_re, r' ', raw)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "    \n",
    "    # 3. Convert to lower case, split into individual words.\n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    # a list, so convert the stopwords to a set.\n",
    "    stops = set(stopwords.words('english'))\n",
    "    \n",
    "    # 5. Remove stopwords.\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return(\" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_df.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hey   !\\n\\nRunning? Lifting? Yoga? Swimming? Rowing? How are you getting your heart rate up these days?\\n\\nShare your fitness regimen OR ask the community any questions you have about working out!\\n\\nIf you're new to    and need some info, start with  (  and  (  Or, if you have a question that doesn't seem to be covered, head on over to the Community Support thread (pinned to the top of the subreddit) and ask the community!\""
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pattern_1 = '\\[(.*?)\\]' #square brackets\n",
    "#pattern_2 = '(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})' #urls\n",
    "#pattern_3 = 'keto' #subreddit(prevent data leakage)\n",
    "#pattern_4 = '\\/r\\/' #indicative of subreddit\n",
    "#generic_re = re.compile(\"(%s|%s|%s|%s)\" % (pattern_1, pattern_2, pattern_3,pattern_4))\n",
    "\n",
    "#raw_text = re.sub(generic_re, r' ', combined_df.iloc[0,0])\n",
    "#raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "995"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_selftext = combined_df.shape[0]\n",
    "total_selftext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 995 reviews.\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {total_selftext} reviews.')\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews.\n",
    "clean_train_selftext = []\n",
    "clean_test_selftext = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the training set articles...\n",
      "Review 100 of 995.\n",
      "Review 200 of 995.\n",
      "Review 300 of 995.\n",
      "Review 400 of 995.\n",
      "Review 500 of 995.\n",
      "Review 600 of 995.\n",
      "Review 700 of 995.\n",
      "Cleaning and parsing the testing set movie reviews...\n",
      "Review 800 of 995.\n",
      "Review 900 of 995.\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleaning and parsing the training set articles...\")\n",
    "\n",
    "# Instantiate counter.\n",
    "j = 0\n",
    "\n",
    "# For every review in our training set...\n",
    "for train_selftext in X_train['selftext']:\n",
    "    \n",
    "    # Convert review to words, then append to clean_train_selftext.\n",
    "    clean_train_selftext.append(selftext_to_words(train_selftext))\n",
    "    \n",
    "    # If the index is divisible by 100, print a message.\n",
    "    if (j + 1) % 100 == 0:\n",
    "        print(f'Review {j + 1} of {total_selftext}.')\n",
    "    \n",
    "    j += 1\n",
    "\n",
    "# Let's do the same for our testing set.\n",
    "print(\"Cleaning and parsing the testing set movie reviews...\")\n",
    "\n",
    "# For every review in our testing set...\n",
    "for test_selftext in X_test['selftext']:\n",
    "    \n",
    "    # Convert review to words, then append to clean_test_selftext.\n",
    "    clean_test_selftext.append(selftext_to_words(test_selftext))\n",
    "    \n",
    "    # If the index is divisible by 100, print a message.\n",
    "    if (j + 1) % 100 == 0:\n",
    "        print(f'Review {j + 1} of {total_selftext}.')\n",
    "        \n",
    "    j += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('logreg', LogisticRegression(solver='lbfgs',max_iter=200))\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = {\n",
    "    'cvec__max_features' :[2000, 3000, 4000, 5000],\n",
    "    'cvec__min_df':[2,3],\n",
    "    'cvec__max_df':[0.9,0.95],\n",
    "    'cvec__ngram_range': [(1, 1),(1, 2),(1, 3)]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, # what object are we optimizing?\n",
    "                  param_grid = pipe_params, # what parameters values are we searching?\n",
    "                  cv=5) # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('cvec',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_accents=None,\n",
       "                                                        token_pattern='(?u)...\n",
       "                                                           penalty='l2',\n",
       "                                                           random_state=None,\n",
       "                                                           solver='lbfgs',\n",
       "                                                           tol=0.0001,\n",
       "                                                           verbose=0,\n",
       "                                                           warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'cvec__max_df': [0.9, 0.95],\n",
       "                         'cvec__max_features': [2000, 3000, 4000, 5000],\n",
       "                         'cvec__min_df': [2, 3],\n",
       "                         'cvec__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(clean_train_selftext,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8794004474272932"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.9,\n",
       " 'cvec__max_features': 4000,\n",
       " 'cvec__min_df': 3,\n",
       " 'cvec__ngram_range': (1, 3)}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_model = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9919571045576407"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_model.score(clean_train_selftext, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9076305220883534"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_model.score(clean_test_selftext, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_tfidf = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('logreg', LogisticRegression(solver='lbfgs',max_iter=200))\n",
    "    \n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = {\n",
    "    'tvec__max_features' :[2000, 3000, 4000, 5000],\n",
    "    'tvec__min_df':[2,3],\n",
    "    'tvec__max_df':[0.9,0.95],\n",
    "    'tvec__ngram_range': [(1, 1),(1, 2),(1, 3)]\n",
    "}\n",
    "\n",
    "gs_tfidf = GridSearchCV(pipe_tfidf, # what object are we optimizing?\n",
    "                  param_grid = pipe_params, # what parameters values are we searching?\n",
    "                  cv=5) # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('tvec',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_acce...\n",
       "                                                           penalty='l2',\n",
       "                                                           random_state=None,\n",
       "                                                           solver='lbfgs',\n",
       "                                                           tol=0.0001,\n",
       "                                                           verbose=0,\n",
       "                                                           warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'tvec__max_df': [0.9, 0.95],\n",
       "                         'tvec__max_features': [2000, 3000, 4000, 5000],\n",
       "                         'tvec__min_df': [2, 3],\n",
       "                         'tvec__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tfidf.fit(clean_train_selftext,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tvec__max_df': 0.9,\n",
       " 'tvec__max_features': 4000,\n",
       " 'tvec__min_df': 2,\n",
       " 'tvec__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tfidf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_model = gs_tfidf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.985254691689008"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_model.score(clean_train_selftext, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9116465863453815"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_model.score(clean_test_selftext, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
